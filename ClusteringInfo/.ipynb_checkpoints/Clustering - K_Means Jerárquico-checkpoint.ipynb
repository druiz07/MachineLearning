{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3huBjyoc_JIf"
   },
   "source": [
    "# Clustering K-means y Jerárquico\n",
    "\n",
    "En esta notebook vamos a explorar dos métodos de clusterización vistos en la clase teórica. Los métodos de clusterización que veremos corresponden al rama de ML de aprendizaje no supervisado donde la idea principal es encontrar grupos en los datos que sean similares a otros datos del mismo grupo y lo menos similar posible a datos en otros. \n",
    "\n",
    "El primer método es el algoritmo de partición **K-Means**, que aplicaremos al dataset de caras Olivetti, trabajado en notebook previas, para tratar de agrupar las fotos según la persona a la que se le sacó la foto. \n",
    "\n",
    "El segundo es el método jerárquico **Hierarchical Clustering**, con la idea de segmentar clientes en distintos grupos basándonos en sus tendencias de compra. El dataset que utilizaremos en este caso es el \"shopping_data.csv\" que está subido a la carpeta de la materia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3p5Og-CtYvG"
   },
   "outputs": [],
   "source": [
    "# importamos las librerías usuales de python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# importamos los algoritmos de clusterizacion a utilizar en esta notebook\n",
    "from sklearn.cluster import KMeans                    # K-means\n",
    "from sklearn.cluster import AgglomerativeClustering   # Clustering jerárquico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgnBn27jytqb"
   },
   "source": [
    "Para agilizar el proceso de clusterización en el dataset de caras vamos identificar las componentes principales como en la clase pasada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJoOO6J9SVR9"
   },
   "outputs": [],
   "source": [
    "# Clase para realizar componentes principales\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Estandarizador (transforma las variables en z-scores)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler() # Creamos el estandarizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2718A0Hop2y"
   },
   "source": [
    "## [K-Means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "\n",
    "El método de clusterización \"K-Means\" busca encontrar *k* clusters de modo tal que se minimice la varianza intra-cluster medida, usualmente, como el cuadrado de la distancia euclídea. Para ello el algoritmo empieza encontrando *k centroids* y le asigna a cada sample la etiqueta correspondiente a la del centroid más cercano. Una vez actualizadas las etiquetas de todas las samples, calcula la posición de los k centroids (como el promedio de las features de las samples de cada cluster) y vuelve a asignar etiquetas a cada sample de acuerdo a la distancia al centroid más cercano. Hace estos dos pasos hasta que no haya más cambios de etiqueta.\n",
    "\n",
    "Debido a esta inicialización aleatoria de los k centroids el output del modelo puede variar al aplicarlo otra vez a la misma data. Por ello se repite este procedimiento n_init=10 veces y el output final es el que mejor resultado tuvo al minimizar la función objetivo (varianza intra-cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jmDt-pjqujR"
   },
   "source": [
    "### Clustering en el dataset de caras Olivetti\n",
    "\n",
    "Importamos el dataset de caras Olivetti como en notebooks anteriores.\n",
    "\n",
    "Recordamos eque el dataset consta de 10 fotos de 64*64 pixeles en escala de grises de 40 personas distintas. Cada sample (foto) importada es un vector de 4096 elementos, los cuales representan las filas de pixeles ordenadas de arriba hacia abajo de la foto. Es decir, los primeros 64 elementos corresponden a la fila superior de pixeles de la imagen y así sucesivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OC7JfXPJS6Yj",
    "outputId": "fbeb9db0-eb81-4ef6-b85d-c9eb2d149a08"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces # para cargar el dataset de caras\n",
    "data, targets = fetch_olivetti_faces(return_X_y = True) # cargamos las caras\n",
    "\n",
    "print('Dimensión de los datos {}'.format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1phUNXUR2UNn"
   },
   "source": [
    "Antes que nada realizamos una descomposición en componentes principales. Notar que todos los features (pixeles) están en la misma escala (de 0 a 255) y por ese motivo no hacemos una normalización previa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pq2SqpoNS8Zn"
   },
   "outputs": [],
   "source": [
    "# Creación del modelo de PCA con 100 componentes\n",
    "pca = PCA(n_components = 100)\n",
    "\n",
    "# Ajuste y transformación de los datos\n",
    "pca.fit(data)\n",
    "X_pca = pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbgMKvla32hn"
   },
   "source": [
    "Chequeamos que la matriz X_pca tiene las dimensiones correspondientes (400 samples, 100 PCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cA4uoHIx323c",
    "outputId": "db295f3f-8ff9-4611-f900-65d2c18bf2a6"
   },
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAU7cap723vg"
   },
   "source": [
    "Ploteamos el dataset en el espacio de las primeras dos componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "DTYy-twRes2X",
    "outputId": "f897ec47-b72c-473e-a103-29fb6655f0d3"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 7))\n",
    "\n",
    "# Hacemos un scatter plot de cada uno de los datos\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "\n",
    "# Por cada dato escribimos a qué instancia corresponde\n",
    "for i in range(data.shape[0]):\n",
    "    ax.text(X_pca[i, 0], X_pca[i, 1], s = i)\n",
    "\n",
    "ax.set_xlabel('Primer componente principal')\n",
    "ax.set_ylabel('Segunda componente principal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb026bWm3Fdz"
   },
   "source": [
    "Ahora sí aplicamos el método de clusterización K-Means para agrupar las fotos en base a sus componentes principales. Como sabemos que el dataset consta de 40 personas diferentes utilizamos esta información para pedirle al algoritmo que encuentre k=40 clusters con la esperanza que podamos agrupar las fotos según la persona que está en ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_H064vuSfl2E",
    "outputId": "c9c94df2-b7ac-4420-f640-69ded50e2e6c"
   },
   "outputs": [],
   "source": [
    "# Creación del modelo KMeans con k = 40\n",
    "kmeans = KMeans(n_clusters=40)\n",
    "\n",
    "# Ajuste del modelo a los datos reducidos en componentes principales\n",
    "kmeans.fit(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca6nFmEe8KsA"
   },
   "source": [
    "Para acceder a las etiquetas que le asignó el modelo a cada sample usamos 'kmeans.labels_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlcDkOQdf0Bn",
    "outputId": "0694ce16-dcd7-4cea-deda-bad2aebcf955"
   },
   "outputs": [],
   "source": [
    "# Nos fijamos las etiquetas asignadas a las primeras 10 muestras y los counts que recibió cada una\n",
    "np.unique(kmeans.labels_[:10], return_counts=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDQpD2ON8YpK"
   },
   "source": [
    "Idealmente queríamos tener una sola etiqueta en las primeras 10 muestras ya que corresponden a las fotos tomadas a una dada persona. Sin embargo obtuvimos 6 etiquetas distintas. \n",
    "\n",
    "Veamos qué es lo que está pasando. Vamos a plotear el dataset en el espacio de las primeras dos componentes principales pero además vamos a asignarle un color a cada sample que corresponde a la etiqueta asignada por el modelo. También vamos a graficar con una \"X\" el centroid de cada cluster.\n",
    "\n",
    "Para acceder a la posición de los centroids en el espacio de 100 PCs usamos 'kmeans.cluster_centers_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gY3dR0koE15i",
    "outputId": "e700995b-35f7-4fe9-ba6f-de2ae2d5002d"
   },
   "outputs": [],
   "source": [
    "# Guardo las posiciones de los centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Printeo las dimensiones de las posiciones\n",
    "print(\"Shape de los centroids:\",centroids.shape)\n",
    "# Printeo las posiciones de las primeras 5 muestras en sus primeras dos componentes principales\n",
    "print(centroids[:5,[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "TuCEXjK2f64W",
    "outputId": "073ae0b2-bc0b-49d5-e477-a30a6dc2ec1e"
   },
   "outputs": [],
   "source": [
    "# Este bloque es similar al anterior pero agregando color a cada sample en el scatter plot según la etiqueta asignada\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20, 7))\n",
    "\n",
    "# Hacemos un scatter plot de cada uno de los datos\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_)\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], marker=\"X\", s=200, linewidths=1,\n",
    "            c=np.unique(kmeans.labels_), edgecolors='black') #, label=['X','Y','Z']\n",
    "ax.legend()\n",
    "\n",
    "## Por cada dato escribimos a qué instancia corresponde\n",
    "#for i in range(data.shape[0]):\n",
    "#    ax.text(X_pca[i, 0], X_pca[i, 1], s = i)\n",
    "\n",
    "ax.set_xlabel('Primer componente principal')\n",
    "ax.set_ylabel('Segunda componente principal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iW45zaek-bGZ"
   },
   "source": [
    "En este plot podemos ver que hay samples más alejados de la nube principal de puntos que tienen un color definido. Sin embargo más al centro del plot vemos que los colores se mezclan y esto se debe a que KMeans funciona bien cuando los grupos tienen forma de una esfera en n_features-dimensiones pero falla en otros casos.\n",
    "\n",
    "Parecería ser que los clusters que encuentra no son los que queríamos que nos diera el modelo. Pero entonces, ¿qué son estos clusters? Plotiemos las imágenes correspondientes a cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6wpZmhGYh_Ul",
    "outputId": "c9e2e25a-e150-4dfc-b670-fcc6b7aab360"
   },
   "outputs": [],
   "source": [
    "# Mostrar el resultado final\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Iniciamos un for con k=40 iteraciones\n",
    "for i in range(40):\n",
    "    index = np.nonzero(labels==i)[0]                                    # los índices correspondientes a la i-ésima etiqueta \n",
    "    num = len(index)                                                    # el número de samples en cada cluster\n",
    "    this_faces = data[index].reshape(len(index),64,64)                  # reshapeamos los samples del i-ésimo cluster para que tenga el formato de una imagen de (64,64) pixeles\n",
    "    fig, axes = plt.subplots(1, num, figsize=(24, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    fig.suptitle(\"Cluster \" + str(i), fontsize=20)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(this_faces[i], cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjYFP1acCwBW"
   },
   "source": [
    "Vemos que varios de los clusters (ej: 11) sí corresponden a las fotos de una persona. Además en casi todos se puede entender por qué el método esta agrupando las fotos (ej: 9, se ven dientes en la sonrisa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0d4eusHq10r"
   },
   "source": [
    "### Elección del número de clusters *k*\n",
    "\n",
    "No hay un consenso en cómo elegir el parámetro *k* para el método KMeans. En algunos casos tenemos una idea de en cuántos grupos deberían estar divididos nuestros datos y simplemente elegimos ese número. Pero en muchos otros casos esta información no la tenemos al momento de hacer el clustering.\n",
    "\n",
    "En esta parte vamos a ver dos approachs para la elección de k:\n",
    "1. Método del codo\n",
    "2. Coeficiente de Silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sLSZZolKzJb"
   },
   "source": [
    "#### 1 [Método del codo](https://en.wikipedia.org/wiki/Elbow_method_(clustering)#:~:text=In%20cluster%20analysis%2C%20the%20elbow,number%20of%20clusters%20to%20use.)\n",
    "\n",
    "Se hace un grafico de la función objetivo en función de la elección de k y se elige el k correspondiente al punto donde agregar un cluster más no baja significativamente el valor que queremos minimzar. Esto se puede detectar a ojo o utilizando algún paquete de python como ['kneed'](https://raghavan.usc.edu//papers/kneedle-simplex11.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalamos el paquete con pip (recomendable usar la terminal de Anaconda)\n",
    "!pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFlHTzCkNQx5",
    "outputId": "089a4944-078e-4d38-ca73-4cf0cb385e7e"
   },
   "outputs": [],
   "source": [
    "from kneed import KneeLocator # importamos el paquete para detectar el codo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBCM9xmNODga"
   },
   "source": [
    "Vamos a aplicar el método KMeans al dataset de caras pero cambiando el número de clusters k y guardaremos el puntaje de la función objetivo, SSE (suma de los cuadrados de la distancia euclidea de cada cluster), en una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP3XAo6wKxtn",
    "outputId": "860c89ac-2d5a-4950-e62c-4b99fc12d716"
   },
   "outputs": [],
   "source": [
    "sse = [] # acá vamos a guardar el puntaje de la función objetivo\n",
    "\n",
    "for k in range(1, 40):\n",
    "    print(k)\n",
    "    kkmeans = KMeans(n_clusters=k)\n",
    "    kkmeans.fit(data)\n",
    "    sse.append(kkmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Toc0L9xiMsEn"
   },
   "source": [
    "Vamos a graficar SEE en función del número de clusters k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "gWCQ_ecnMfY5",
    "outputId": "d87af40c-0016-444c-9d92-598529f1ac9d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 7))\n",
    "\n",
    "# esta dos lineas las agrego para que se vea la elección de KneeLocator para el codo en este gráfico\n",
    "ax.scatter(8, sse[7], color='red', s=200) # agregamos un punto rojo al plot de tamaño s=200 en el lugar donde se encuentra el codo\n",
    "ax.text(7.5, sse[7]-1000, s=\"codo\")       # agregamos un texto abajo para indicar qué representa el punto\n",
    "\n",
    "# estas lineas son el grafico de SSEvsK\n",
    "ax.scatter(range(1, 40), sse)            \n",
    "ax.set_xticks(range(1, 40))\n",
    "ax.set_xlabel(\"Número de clusters\")\n",
    "ax.set_ylabel(\"SSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiYXOQO1SUcB"
   },
   "source": [
    "En este plot podemos ver dos cosas:\n",
    "1. SSE es monótonamente decreciente: al agregar un cluster la distancia entre todas las samples a los centroides siempre va a reducirse\n",
    "2. A ojo distinguimos un codo entre k=5 y k=9 porque al agregar más clusters aumentamos la complejidad del modelo pero SEE disminuye en menor proporción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhWHLdlITyAb"
   },
   "source": [
    "Usamos la función 'KneeLocator' para detectar el codo. Para ello le tenemos que pasar los valores de K, SEE, la forma de la fución (cóncava o convexa) y la dirección (creciente o decreciente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVYQi5TGNGiG",
    "outputId": "c4a9789e-cb60-4a9b-e386-df05e5eca698"
   },
   "outputs": [],
   "source": [
    "kl = KneeLocator(range(1, 40), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "print(\"El codo está en k =\", kl.elbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlRKHgI6Uuly"
   },
   "source": [
    "Veamos entonces cómo se ve el modelo con k=8 para el dataset de caras Olivetti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "iVIl1atBsQlG",
    "outputId": "3b115036-dddc-482e-9f21-810909a3c085"
   },
   "outputs": [],
   "source": [
    "# Creación del modelo KMeans con k = 8\n",
    "kmeans8 = KMeans(n_clusters=8)\n",
    "\n",
    "# Ajuste del modelo a los datos reducidos en componentes principales\n",
    "kmeans8.fit(X_pca)\n",
    "\n",
    "# Guardamos la posición de los centroids\n",
    "centroids8 = kmeans8.cluster_centers_\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize = (20, 7))\n",
    "\n",
    "# Hacemos un scatter plot de cada uno de los datos\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans8.labels_)\n",
    "ax.scatter(centroids8[:, 0], centroids8[:, 1], marker=\"X\", s=200, linewidths=2,\n",
    "            c=np.unique(kmeans8.labels_),edgecolors='black')\n",
    "ax.legend()\n",
    "\n",
    "## Por cada dato escribimos a qué instancia corresponde\n",
    "#for i in range(data.shape[0]):\n",
    "#  ax.text(X_pca[i, 0], X_pca[i, 1], s = i)\n",
    "\n",
    "ax.set_xlabel('Primer componente principal')\n",
    "ax.set_ylabel('Segunda componente principal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVLe-iflos_Y"
   },
   "source": [
    "#### 2 Coeficiente de Silhouette\n",
    "\n",
    "El coeficiente de Silhouette mide qué tan cerca está una muestra a las otras muestras de su cluster y qué tan lejos está con respecto a las muestras del cluster más cercano. Este coeficiente toma valores de [-1,1], -1 sería si los clusters están superpuestos, 0 si hay overlap y 1 que no se tocan.\n",
    "\n",
    "El coeficiente de Silhouette de cada sample la podemos obtener con la clase 'silhouette_samples' de sklearn.metrics\n",
    "\n",
    "El puntaje de Silhouette es el promedio de los coeficientes de Silhouette de todas las samples y se computa con la clase 'silhouette_score' de sklearn.metrics. Hay que pasarle a la función los datos y sus etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGBz1g20owwS"
   },
   "outputs": [],
   "source": [
    "# importamos el puntaje de silhouette\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vabTae7Ca3-f"
   },
   "outputs": [],
   "source": [
    "# Creamos una lista para guardar de los coeficientes de silhouette para cada valor de k\n",
    "silhouette_coefficients = []\n",
    "\n",
    "# Se necesita tener al menos 2 clusters y a los sumo N-1 (con N el numero de muestras) para obtener coeficientes de Silohuette\n",
    "for k in range(2, 20):\n",
    "    kkkmeans = KMeans(n_clusters=k)\n",
    "    kkkmeans.fit(data)\n",
    "    score = silhouette_score(data, kkkmeans.labels_)\n",
    "    silhouette_coefficients.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJwSyISBgo3o"
   },
   "source": [
    "Graficamos el puntaje de Silhouette en función de k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "DsW2kdAKbQqk",
    "outputId": "0fd6db21-5bbd-4c38-fdf3-79fef13a7c64"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (24, 7))\n",
    "\n",
    "# estas lineas son el grafico de SSEvsK\n",
    "ax.scatter(range(2, 20), silhouette_coefficients)            \n",
    "ax.set_xticks(range(2, 20))\n",
    "ax.set_xlabel(\"Número de clusters\")\n",
    "ax.set_ylabel(\"Promedio coeficientes de Silhouette\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpBEcGytgsYD",
    "outputId": "18aa60fa-00ba-4938-c26e-cf6fd637309e"
   },
   "outputs": [],
   "source": [
    "kklabels = kkkmeans.labels_\n",
    "kkklabels = kklabels.copy()\n",
    "\n",
    "np.random.shuffle(kkklabels)\n",
    "\n",
    "silhouette_score(data, kkklabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5WXcE0HZZEv"
   },
   "source": [
    "En este plot vemos que el puntaje de Silhouette < 0.2 y tiene poca desviación. \n",
    "\n",
    "Se puede decir que **no se encuentran estructuras fuertes en los datos** y es necesaria una *inspección manual* de ellos para determinar el número óptimo de clusters (y/o probar con otros métodos). Sin embargo en muchos dataset reales va a pasar que no todo se puede hacer de manera automática y se debe tener intuición o noción de lo que están representando los datos para un realizar un análisis correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdxjX8EWNcD9"
   },
   "source": [
    "## [Clustering Jerárquico](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)\n",
    "\n",
    "Los métodos de clustering jerárquico buscan agrupar las samples más similares para formar grupos con características similares.\n",
    "\n",
    "1. **Agglomerative**: cada sample es un cluster y en cada paso va agrupando los clusters más similares hasta quedarse con un solo cluster\n",
    "2. **Divisive**: todas las samples comienzan en el mismo cluster y en cada paso va cortando las samples menos similares hasta que todas las samples sean un cluster distinto\n",
    "\n",
    "En esta notebook vamos a ver el primer tipo de clustering jerárquico aplicado a un dataset de tendencias de compra de clientes de un shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "wpX6fgZnkW7u",
    "outputId": "0e433647-d6f4-43c2-f188-0e8bc06ff2ef"
   },
   "outputs": [],
   "source": [
    "# Path de nuestro dataset\n",
    "filename = './data/shopping_data.csv'\n",
    "\n",
    "# Carga del dataset\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Inspecciono las primeras filas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGD6IVQtlMyY",
    "outputId": "1cf0ba60-39f1-4716-8aee-3532e1dd8cb4"
   },
   "outputs": [],
   "source": [
    "print(\"Dimensiones del dataset:\",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6LjWltunKx9"
   },
   "source": [
    "Este dataset tiene de features una ID asociada a cada uno de los 200 clientes, el género binario, la edad de la persona, el ingreso anual (en miles de dolares) y un puntaje asociado a cuán a menudo un cliente gasta plata en el shopping siendo 100 el cliente que más gasta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjsuqTEKhhpF"
   },
   "source": [
    "Vamos a tratar de agrupar a los clientes utilizando solo dos features del dataset: su ganancia anual y el puntaje de gastos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdwcUNRglSk8",
    "outputId": "8264cb22-5005-4c73-b8b9-59e3958d72d1"
   },
   "outputs": [],
   "source": [
    "# Me armo una matriz con las ultimas dos columnas del dataframe\n",
    "sdata = df.iloc[:,3:5].values\n",
    "\n",
    "sdata[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC8EPpghn0iw"
   },
   "source": [
    "Importamos un paquete de scipy que nos va a ayudar a plotear dendogramas. Lo importamos entero porque vamos a usar dos de sus clases 'dendogram' y 'linkage'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aIt5GcqmzZP"
   },
   "outputs": [],
   "source": [
    "# Paquete de scipy que tiene la clase 'dendograma' que vamos a utilizar\n",
    "import scipy.cluster.hierarchy as shc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFQZ8o6Ei6ZN"
   },
   "source": [
    "Vamos a realizar un dendograma del dataset utilizando el método 'ward' para calcular distancias (es el que se suele ultilizar y viene por default). Este método minimiza la varianza dentro de un cluster y maxima la varianza entre clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "ljJRUhmWmsIu",
    "outputId": "2229f7b3-e37e-4322-c4cf-7728faa1cb93"
   },
   "outputs": [],
   "source": [
    "# Plot del dendograma del dataset de clientes\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Dendograma de clientes\")\n",
    "plt.ylabel(\"Distancia ward\")\n",
    "\n",
    "# Con la función 'dendogram' graficamos el dendograma. \n",
    "# El input de esta función es la función 'linkage' donde se especifica la distancia para utlizar en cada paso del método\n",
    "dend = shc.dendrogram(shc.linkage(sdata, method='ward'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjHpeaNlmYG1"
   },
   "source": [
    "Lo que buscamos en el dendograma es la mayor distancia vertical sin que haya una línea horizontal para hacerle un corte (representado como una linea horizontal que cruza todos los datos) y quedarnos con k clusters (donde k es el número de lineas verticales que intersectan el corte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "hiltUQzIovAO",
    "outputId": "56c45138-28fa-4477-e341-6669fc1c71c9"
   },
   "outputs": [],
   "source": [
    "# Plot del dendograma del dataset de clientes\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Dendograma de clientes\")\n",
    "plt.ylabel(\"Distancia ward\")\n",
    "\n",
    "# Con la función 'dendogram' graficamos el dendograma. \n",
    "# El input de esta función es la función 'linkage' donde se especifica la distancia para utlizar en cada paso del método\n",
    "dend = shc.dendrogram(shc.linkage(sdata, method='ward'))  \n",
    "plt.axhline(150, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93ZFHFN4pEVU"
   },
   "source": [
    "Vemos que el corte intersecta con 5 líneas verticales por lo que vamos a utilizar el método de clustering jerárquico de sklearn con n_clusters=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-YbZLWbnSDF"
   },
   "outputs": [],
   "source": [
    "# Importo el método de clustering jerárquico (bottom-up)\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESigJy16mLuo"
   },
   "source": [
    "Ahora sí aplicamos el método de clusterización jerárquica (bottom-up) con 5 clusters, la distancia euclidea para la afinidad y la distancia ward para el linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uquRzVNnSwr",
    "outputId": "cb3d2d84-01d6-4a56-ebaf-4396f87ddb39"
   },
   "outputs": [],
   "source": [
    "# Creamos el modelo\n",
    "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "\n",
    "# Lo ajustamos con los datos\n",
    "cluster.fit_predict(sdata)  # fit_predict hace lo mismo que fit pero devuelve el vector de etiquetas de las samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bjj97NRqq5C"
   },
   "source": [
    "Veamos cómo se ven los datos agrupados en el espacio de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "nSOUDCtlnZK8",
    "outputId": "94b9fdd4-5029-4212-d313-09e5d4df5754"
   },
   "outputs": [],
   "source": [
    "# Ploteamos los datos en el espacio de (Ingresos,Gastos) con un color por cada uno de los 5 clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Dataset de clientes de un shopping\")\n",
    "plt.scatter(sdata[:,0], sdata[:,1], c=cluster.labels_, cmap='rainbow')\n",
    "plt.xlabel(\"Ingereso anual (k$)\")\n",
    "plt.ylabel(\"Puntaje de gastos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZBl_7O4rRKN"
   },
   "source": [
    "El cluster violeta, abajo a la derecha, corresponde a clientes con salarios altos pero que gastan poco en el shopping. Estos son clientes que gastan con cuidado su dinero.\n",
    "\n",
    "El cluster verde clarito representa a clientes con mucho salario y que gastan hasta el último centavo en bobadas. Estos son los tipos de clientes que una compañía busca para llenarlos de publicidad.\n",
    "\n",
    "El cluster celeste es el de clientes 'promedio'. El mayor numero de samples cae dentro de este cluster y por eso también son el objetivo de compañías que buscan llegar a mucha gente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgtAEYR8Zu-q"
   },
   "source": [
    "## Resumen de las principales funciones en Python\n",
    "\n",
    "**KMeans**\n",
    "\n",
    "```\n",
    "from sklearn.cluster import KMeans  # Importamos la clase KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=k)       # Crear el modelo con k-clusters\n",
    "\n",
    "kmeans.fit(datos)                   # Ajustar el modelo a los datos\n",
    "\n",
    "labels = kmeans.labels_             # Obtener las etiquetas de los k clusters para cada sample\n",
    "\n",
    "centroids = kmeans.cluster_centers_ # Obtener las posiciones en el espacio de features del centro de los k clusters\n",
    "\n",
    "sse = kmeans.inertia_               # Para obtener el valor de la función objetivo\n",
    "```\n",
    "\n",
    "Elección de k (si no lo conocemos de antemano):\n",
    "\n",
    "**Método del codo**\n",
    "\n",
    "```\n",
    "!pip install kneed            # Instalamos el paquete kneed con pip install\n",
    "\n",
    "from kneed import KneeLocator # Importamos la clase para detectar el codo\n",
    "\n",
    "kl = KneeLocator(range(1, 40), sse, curve=\"convex\", direction=\"decreasing\") # Clase que sirve para calcular el codo en un gráfico de la función objetivo. Se le dan los puntos de la función con sus respectivos k, el tipo de curva (convexa o cóncava) y el sentido (creciente o decreciente)\n",
    "\n",
    "kl.elbow                      # Devuelve el valor del codo\n",
    "```\n",
    "\n",
    "**Puntaje de Silhouette**\n",
    "\n",
    "```\n",
    "from sklearn.metrics import silhouette_samples # importamos la clase que computa el coeficiente de silhouette para cada sample\n",
    "\n",
    "from sklearn.metrics import silhouette_score # importamos la clase con la que se calcula el puntaje de silhouette (que es el promedio de los coeficientes de silhouette para cada sample)\n",
    "\n",
    "silhouette_score(data, kkkmeans.labels_) # Devuelve el puntaje de silhouette y toma como input los datos y las etiquetas asociadas\n",
    "```\n",
    "\n",
    "**Clustering Jerárquico**\n",
    "\n",
    "```\n",
    "import scipy.cluster.hierarchy as shc # Paquete de scipy que tiene la clase 'dendograma' y 'linkage' que vamos a utilizar\n",
    "\n",
    "dend = shc.dendrogram(shc.linkage(data, method='ward')) # Para hacer un dendograma de los datos usnado como linkage la distancia ward\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering # Importo el método de clustering jerárquico (bottom-up)\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward') # Creamos el modelo\n",
    "\n",
    "cluster.fit_predict(sdata)  # Ajustamos el modelo. '.fit_predict' hace lo mismo que '.fit' pero imprime el vector de etiquetas de las samples\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Clase 16:  Clustering K-means y Jerárquico.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
