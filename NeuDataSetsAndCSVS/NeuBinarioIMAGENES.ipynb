{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CARGA DE IMAGENES DE CLASIFICACION BINARIA / INICIALIZACION DE LA RED CON NEURONAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from RedNeu import cost, backprop , predict,initialize_parameters,update_parameters\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.io import loadmat, savemat\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# En caso de solo tener un data set /Habria que mencionar que se podria estar usando kfold validation\n",
    "\n",
    "##data = loadmat('imageset.mat', squeeze_me=True)\n",
    "##X= pd.DataFrame(data['X'])\n",
    "##y=pd.DataFrame(data['y'])\n",
    "# X, X2, y, y2 = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "data = loadmat('trainset.mat', squeeze_me=True)\n",
    "\n",
    "data2 = loadmat('testset.mat', squeeze_me=True)\n",
    "\n",
    "X= pd.DataFrame(data['X'])\n",
    "X2= pd.DataFrame(data2['Xtest']);\n",
    "y=pd.DataFrame(data['y'])\n",
    "y2=pd.DataFrame(data2['ytest'])\n",
    "\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X2 = scaler.transform(X2)\n",
    "\n",
    "# Añadir el sesgo a los conjuntos de entrenamiento y prueba\n",
    "X_train_with_bias = np.hstack([np.ones((len(X), 1)), X])\n",
    "X_test_with_bias = np.hstack([np.ones((len(X2), 1)), X2])\n",
    "\n",
    "\n",
    "##Tantas neuronas de entrada como datos , y de salida si es binaria 1( o 1 o 0) y si es multi las que tenga y\n",
    "layer_sizes = [X.shape[1] ,10,1]  # Ajusta el número de neuronas según sea necesario\n",
    "theta_list = initialize_parameters(layer_sizes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ENTRENAMIENTO DEL MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajustar hiperparámetros\n",
    "lambdaValue = 0.35  # Experimenta con diferentes valores\n",
    "alpha = 0.1  # Experimenta con diferentes valores\n",
    "iterations = 500  # Experimenta con diferentes valores\n",
    "\n",
    "\n",
    "\n",
    "# Paso 3: Entrenamiento del perceptrón multicapa\n",
    "for _ in range(iterations):\n",
    "    J, grad = backprop(theta_list, X_train_with_bias, y, lambdaValue)\n",
    "    # Actualizar parámetros\n",
    "    update_parameters(theta_list, grad, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediccion de los datos y calculo de metricas de medida de rendimiento**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paso 4: Evaluación del perceptrón multicapa\n",
    "predictions_test = predict(theta_list, X_test_with_bias)\n",
    "\n",
    "threshold = 0.5; ##Umbral de calculo de prediccion\n",
    "my_predict = np.array(predictions_test) >= threshold\n",
    "my_predict = my_predict.astype(int);\n",
    "\n",
    "accuracy_train = accuracy_score(y2, my_predict)\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix_test = confusion_matrix(y2, my_predict)\n",
    "cost_train = cost(theta_list, X_train_with_bias, y, lambdaValue)\n",
    "cost_test = cost(theta_list, X_test_with_bias, y2, lambdaValue)\n",
    "\n",
    "\n",
    "# Obtener True Positives (Verdaderos Positivos) y False Positives (Falsos Positivos) de la matriz de confusión\n",
    "##ESTO SE HACE ASI POR QUE ES CLASIFICACION BINARIA ,EN ESTE EJEMPLO:  O PERSONA O CABALLO \n",
    "##SI ESTO FUESE MULTICLASE HABRIA QUE HACER UNA CLASE POSITIVA PARA CADA ETIQUETA\n",
    "# Obtener True Positives (Verdaderos Positivos) y False Positives (Falsos Positivos) de la matriz de confusión\n",
    "true_positives_class_1 = conf_matrix_test[1, 1]  # Posición [1, 1] para la clase positiva (1)\n",
    "false_positives_class_1 = conf_matrix_test[0, 1]  # Posición [0, 1] para la clase negativa (0) clasificada como positiva (1)\n",
    "# Obtener True Negatives (Verdaderos Negativos) y False Negatives (Falsos Negativos) de la matriz de confusión\n",
    "true_negatives_class_0 = conf_matrix_test[0, 0]  # Posición [0, 0] para la clase negativa (0)\n",
    "false_negatives_class_0 = conf_matrix_test[1, 0]  # Posición [1, 0] para la clase positiva (1) clasificada como negativa (0)\n",
    "\n",
    "\n",
    "\n",
    "# Calcular Precision para la clase positiva (1)\n",
    "precision_class_1 = true_positives_class_1 / (true_positives_class_1 + false_positives_class_1) if (true_positives_class_1 + false_positives_class_1) > 0 else 0\n",
    "# Calcular Precision para la clase negativa (0)\n",
    "precision_class_0 = true_negatives_class_0 / (true_negatives_class_0 + false_negatives_class_0) if (true_negatives_class_0 + false_negatives_class_0) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPRESION DE LOS RESULTADOS OBTENIDOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costo de la red neuronal en entrenamiento: 0.13 %\n",
      "Costo de la red neuronal en prueba: 0.51 %\n",
      "Accuracy : 78.52%\n",
      "Precision Para la clase Positiva (1): 86.14%\n",
      "Precision Para la clase Negativa (0): 73.55%\n",
      "Matriz de Confusión Test:\n",
      "[[114  14]\n",
      " [ 41  87]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Costo de la red neuronal en entrenamiento: {cost_train.values[0]:.2f} %')\n",
    "print(f'Costo de la red neuronal en prueba: {cost_test.values[0]:.2f} %')\n",
    "print(f'Accuracy : {accuracy_train * 100:.2f}%')\n",
    "# SE HACE ASI POR QUE SOLO HAY DOS CLASES ES BINARIA , SI FUESE MULTICLASE PONRIAMOS LAS QUE NOS INTERESEN\n",
    "# Imprimir Precision para ambas clases\n",
    "print(f'Precision Para la clase Positiva (1): {precision_class_1 * 100:.2f}%')\n",
    "print(f'Precision Para la clase Negativa (0): {precision_class_0 * 100:.2f}%')\n",
    "# Imprimir la matriz de confusión\n",
    "print(\"Matriz de Confusión Test:\")\n",
    "print(conf_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados del Perceptrón Multicapa de Scikit-Learn:\n",
      "Accuracy: 85.16%\n",
      "Matriz de Confusión Test:\n",
      "[[ 94  34]\n",
      " [  4 124]]\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.73      0.83       128\n",
      "           1       0.78      0.97      0.87       128\n",
      "\n",
      "    accuracy                           0.85       256\n",
      "   macro avg       0.87      0.85      0.85       256\n",
      "weighted avg       0.87      0.85      0.85       256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##PERCEPTRON DE SKLEARN\n",
    "from sklearn.calibration import column_or_1d\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "##IMPORTANTE : SOBRE ESTE MODELO NO AÑADIMOS EL SESGO \n",
    "##EL MODELO INTEPRETA AUTOMATICAMENTE EL NUMERO DE CAPAS DE ENTRADA Y DE SALIDA \n",
    "\n",
    "mlp_sklearn = MLPClassifier(\n",
    "    hidden_layer_sizes=(10,),  # Ajustar el número de neuronas en la capa oculta según sea necesario\n",
    "    activation='relu',        # Usar la función de activación ReLU \n",
    "    max_iter=500,             # Número máximo de iteraciones\n",
    "    alpha=0.1,               # Término de regularización\n",
    "    learning_rate_init=0.35,   # Tasa de aprendizaje inicial\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "mlp_sklearn.fit(X, y.values.ravel())  # Ajustar el modelo,\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions_sklearn = mlp_sklearn.predict(X2)\n",
    "\n",
    "\n",
    "# Calcular métricas de evaluación del modelo de Scikit-Learn\n",
    "accuracy_sklearn = accuracy_score(y2, predictions_sklearn)\n",
    "conf_matrix_sklearn = confusion_matrix(y2, predictions_sklearn)\n",
    "# Calcular el informe de clasificación\n",
    "report_sklearn = classification_report(y2, predictions_sklearn)\n",
    "\n",
    "print(\"\\nResultados del Perceptrón Multicapa de Scikit-Learn:\")\n",
    "print(f'Accuracy: {accuracy_sklearn * 100:.2f}%')\n",
    "print(\"Matriz de Confusión Test:\")\n",
    "print(conf_matrix_sklearn)\n",
    "print(\"Informe de Clasificación:\")\n",
    "print(report_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicaciones sobre el uso de la red neuronal\n",
    "\n",
    "Este código implementa una red neuronal para un problema de clasificación binaria. A continuación, explicaré las funciones y parámetros:\n",
    "\n",
    "## 1. Funciones de activación:\n",
    "\n",
    "- `sigmoid(z)`: Devuelve el valor de la función sigmoide para la entrada 'z'.\n",
    "- `sigmoid_gradient(z)`: Devuelve la derivada de la función sigmoide para la entrada 'z'.\n",
    "\n",
    "\n",
    "## 2. Función de costo con regularización:\n",
    "\n",
    "- `cost(theta_list, X, Y, lambda_)`: Calcula la función de costo para la red neuronal. Utiliza la entropía cruzada para la clasificación binaria y agrega términos de regularización para evitar el sobreajuste.\n",
    "\n",
    "## 3. Función de predicción:\n",
    "\n",
    "- `predict(theta_list, X)`: Realiza la propagación hacia adelante y devuelve las predicciones de la red neuronal.\n",
    "\n",
    "## 4. Inicialización de parámetros:\n",
    "\n",
    "- `initialize_parameters(layer_sizes)`: Inicializa los pesos de la red neuronal de manera aleatoria. 'layer_sizes' especifica el número de neuronas en cada capa.\n",
    "\n",
    "## 5. Propagación hacia adelante:\n",
    "\n",
    "- `forward_propagation(X, theta_list)`: Realiza la propagación hacia adelante y devuelve las activaciones y valores de 'z' para cada capa.\n",
    "\n",
    "## 6. Actualización de parámetros:\n",
    "\n",
    "- `update_parameters(theta_list, grads, learning_rate)`: Actualiza los pesos de la red neuronal utilizando el descenso de gradiente.\n",
    "\n",
    "## 7. Propagación hacia atrás (Backpropagation):\n",
    "\n",
    "- `backprop(theta_list, X, y, lambda_)`: Calcula el costo y los gradientes utilizando la propagación hacia atrás. Los gradientes se utilizan para la actualización de parámetros.\n",
    "\n",
    "## Configuración de parámetros:\n",
    "\n",
    "- `layer_sizes`: Se define como '[n_input, n_output]', donde 'n_input' es el número de características de entrada y 'n_output' es el número de neuronas en la capa de salida.\n",
    "- `lambda_`: Parámetro de regularización. Controla la importancia de los términos de regularización en la función de costo.\n",
    "- `learning_rate`: Tasa de aprendizaje para el descenso de gradiente.\n",
    "- `iterations`: Número de iteraciones del descenso de gradiente.\n",
    "\n",
    "\n",
    "**Tasa de Aprendizaje (Learning Rate):**\n",
    "\n",
    "La tasa de aprendizaje es un parámetro que determina qué tan grandes son los pasos que la red neuronal toma durante el descenso de gradiente. Aquí hay algunas consideraciones al subir o bajar la tasa de aprendizaje:\n",
    "\n",
    "- *Subir la Tasa de Aprendizaje:*\n",
    "  - **Ventajas:** Puede conducir a convergencia más rápida durante el entrenamiento.\n",
    "  - **Desventajas:** Si la tasa de aprendizaje es demasiado alta, el descenso de gradiente puede oscilar o divergir, lo que significa que los pesos pueden no converger a un mínimo global, y el entrenamiento puede volverse inestable.\n",
    "\n",
    "- *Bajar la Tasa de Aprendizaje:*\n",
    "  - **Ventajas:** Puede mejorar la estabilidad del entrenamiento y permitir una convergencia más suave.\n",
    "  - **Desventajas:** Demorará más en converger, especialmente en conjuntos de datos grandes. Si la tasa de aprendizaje es demasiado baja, el modelo puede tardar demasiado en aprender y puede quedar atrapado en mínimos locales.\n",
    "\n",
    "**Recomendaciones:**\n",
    "- Comienza con una tasa de aprendizaje moderada y ajústala según sea necesario.\n",
    "- Utiliza técnicas como la disminución adaptativa del learning rate para ajustar dinámicamente la tasa de aprendizaje durante el entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "**Parámetro de Regularización (Alfa o Lambda):**\n",
    "\n",
    "El parámetro de regularización controla la magnitud de la penalización aplicada a los pesos de la red para prevenir el sobreajuste. Aquí hay algunas consideraciones al subir o bajar el parámetro de regularización:\n",
    "\n",
    "- *Subir el Parámetro de Regularización:*\n",
    "  - **Ventajas:** Puede ayudar a prevenir el sobreajuste al penalizar pesos grandes.\n",
    "  - **Desventajas:** Si es demasiado alto, puede llevar a una supresión excesiva de los pesos, haciendo que el modelo sea demasiado simple y no capte patrones complejos en los datos.\n",
    "\n",
    "- *Bajar el Parámetro de Regularización:*\n",
    "  - **Ventajas:** Puede permitir que el modelo capture patrones más complejos en los datos.\n",
    "  - **Desventajas:** Aumenta el riesgo de sobreajuste, especialmente en conjuntos de datos pequeños o cuando la complejidad del modelo es alta.\n",
    "\n",
    "**Recomendaciones:**\n",
    "- Utilizo validación cruzada para encontrar el valor óptimo del parámetro de regularización.\n",
    "- Experimento con valores en una escala logarítmica (0.1, 0.01, 0.001, etc.) para encontrar el equilibrio adecuado entre regularización y capacidad del modelo.\n",
    "\n",
    "\n",
    "## Uso de la codificación one-hot para problemas multiclase:\n",
    "\n",
    "- Cuando se enfrenta a un problema de clasificación multiclase, la codificación one-hot se aplica a las etiquetas de clase. Esta codificación convierte las etiquetas de clase en vectores binarios, donde cada vector representa una clase única y la posición del 1 indica la clase.\n",
    "- La función de costo y las funciones de actualización de parámetros han sido diseñadas para manejar problemas de clasificación binaria y multiclase mediante el uso de one-hot encoding.\n",
    "\n",
    "## Análisis:\n",
    "\n",
    "- La red neuronal es adecuada tanto para problemas de clasificación binaria como multiclase.\n",
    "- La codificación one-hot aborda eficazmente el problema multiclase y permite que la red clasifique instancias en más de dos clases.\n",
    "- Es importante ajustar la tasa de aprendizaje y el término de regularización para obtener un rendimiento óptimo en problemas multiclase.\n",
    "- Las redes neuronales, especialmente las más profundas, tienden a necesitar grandes cantidades de datos para entrenarse de manera efectiva. Si tienes un conjunto de datos relativamente pequeño, podrías enfrentar problemas de sobreajuste.\n",
    "- La red neuronal también se puede mejorar mediante el uso de validación cruzada.\n",
    "\n",
    "## Esta configuración de la red resuelve problemas de tres tipos:\n",
    "\n",
    "1. **Problema Multiclase String**: Ejemplo de clasificación de acciones (\"Action\", \"Accelerate\", etc.).\n",
    "2. **Problema Multiclase Numérico**: Ejemplo de clasificación de calidad del vino (0-9).\n",
    "3. **Problemas de Clasificación Binaria**: Ejemplo de clasificación de si el vino es bueno o no.\n",
    "4.  **Problema de Clasificacion de Imagenes** Otro tipo de problema de  clasificacion , en archivos . mat\n",
    "\n",
    "### Modelos Recomendados:\n",
    "# 1. Clasificación Binaria: Redes Neuronales con Capas Ocultas\n",
    "## Motivos:\n",
    "- **Aprendizaje de Representaciones Complejas:** Las redes neuronales permiten aprender representaciones no lineales y complejas de los datos. En problemas de clasificación binaria, donde las relaciones entre las características y la salida pueden ser no lineales, las capas ocultas de la red neuronal pueden capturar patrones más sofisticados.\n",
    "- **Extracción de Características Abstractas:** Las capas ocultas posibilitan la extracción de características más abstractas y complejas. Esto es esencial cuando las relaciones subyacentes en los datos no son evidentes y requieren representaciones más profundas para su comprensión.\n",
    "- **Manejo de Patrones No Lineales:** Las redes neuronales son especialmente aptas para manejar patrones no lineales en los datos. En problemas de clasificación binaria, donde las decisiones de clasificación pueden estar basadas en relaciones complejas, las redes neuronales pueden superar las limitaciones de modelos más simples como la regresión logística.\n",
    "\n",
    "# 2. Clasificación de Imagenes: Redes Neuronales Convolucionales (CNN)\n",
    "## Motivos:\n",
    "- **Efectividad en Clasificación de Imágenes:** Las CNN son altamente efectivas en la clasificación de datos de imágenes. Están diseñadas para aprender jerarquías de características, desde características simples hasta más complejas, lo que es esencial para interpretar datos visuales.\n",
    "- **Captura de Patrones Espaciales y de Textura:** Las CNN son capaces de capturar patrones espaciales y de textura en datos bidimensionales como imágenes. La arquitectura convolucional permite la extracción eficiente de características locales, lo que es crucial en problemas de clasificación de texto.\n",
    "- **Reducción de Parámetros con Compartición de Pesos:** La arquitectura convolucional reduce el número de parámetros al compartir pesos. Esto es beneficioso en conjuntos de datos grandes como imágenes, donde se pueden aprender patrones locales que son útiles en diferentes partes de la imagen.\n",
    "\n",
    "# 3. Clasificación Multiclase : Regresión Logística o Redes Neuronales FeedFoward\n",
    "## Motivos:\n",
    "- **Eficiencia de la Regresión Logística:** La regresión logística es simple y efectiva para problemas de clasificación multiclase. Cuando la relación entre las características y la salida es relativamente lineal, la regresión logística puede ser una opción eficiente.\n",
    "- **Adaptabilidad de las Redes Neuronales:** Las redes neuronales pueden adaptarse mejor a patrones no lineales y relaciones complejas en comparación con la regresión logística. Si el problema presenta una complejidad significativa, las redes neuronales pueden capturar mejor estas relaciones.\n",
    "- **Elección Dependiendo del Tamaño del Conjunto de Datos:** La elección entre regresión logística y redes neuronales puede depender del tamaño del conjunto de datos. En conjuntos de datos pequeños, la regresión logística puede ser menos propensa al sobreajuste.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Comparaciones y Consideraciones Generales:\n",
    "- **Tamaño del Conjunto de Datos:**\n",
    "  - *Conjuntos Pequeños:* Modelos más simples como la regresión logística pueden ser preferibles para evitar sobreajuste en conjuntos de datos pequeños.\n",
    "  - *Conjuntos Grandes:* Modelos más complejos como las redes neuronales pueden aprovechar mejor conjuntos de datos grandes para aprender patrones más complejos.\n",
    "- **Complejidad del Problema:**\n",
    "  - *No Linealidades y Complejidades:* Problemas con patrones no lineales y relaciones complejas entre las características pueden beneficiarse de modelos más complejos como las redes neuronales.\n",
    "- **Interpretabilidad:**\n",
    "  - *Claridad de Coeficientes:* La regresión logística ofrece una interpretación clara de los coeficientes para cada característica, lo que puede ser crucial en aplicaciones donde la interpretabilidad es fundamental.\n",
    "  - *Complejidad de las Redes Neuronales:* Las redes neuronales pueden ser menos interpretables debido a su complejidad, y la interpretación de cómo las características contribuyen a las decisiones puede ser más desafiante.\n",
    "- **Tiempo de Entrenamiento:**\n",
    "  - *Modelos Complejos:* Modelos más complejos, como las redes neuronales profundas, pueden requerir más tiempo de entrenamiento, especialmente en conjuntos de datos grandes. Esto debe considerarse en aplicaciones donde el tiempo de entrenamiento es un factor crítico.\n",
    "- **Validación Cruzada:**\n",
    "  - *Evaluación del Rendimiento:* La validación cruzada es crucial para evaluar el rendimiento del modelo en diferentes conjuntos de datos y evitar el sobreajuste. Es especialmente importante en problemas con conjuntos de datos limitados.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
